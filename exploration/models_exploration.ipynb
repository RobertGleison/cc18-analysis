{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated: \n",
    "This file is no more utilized. The members of the project decided to move on the analysis only with KNN models. To extract data and informations of the datasets, we now utilize the 'knn_exploration.ipynb' or 'knn_exploration.py', which provide better logs, more informations about knn and a simple code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "<div style=\"text-align: justify\">\n",
    "In this phase, we colect various metrics including precision, F1 score, recall, accuracy, and ROC AUC OVR across different models such as decision trees, KNNs, and SVMs. We employ GridSearchCV to explore combinations of hyperparameters. The goal is to assess a CSV containing metrics for each model and hyperparameter set, discerning their performance variations across tasks sourced from the OpenML repository.\n",
    "\n",
    "To streamline the process, we partition the OpenML-CC18 Curated classification dataset into segments, distinguishing between multiclass datasets, balanced binary datasets, and imbalanced binary datasets. We employ a threshold criterion, set at 0.3, to determine whether a binary dataset is balanced or imbalanced. Specifically, if one class constitutes less than or equal to 30% of the total targets, the dataset is classified as imbalanced.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "# logging.basicConfig(filename='benchmark.log', level=logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function separate_dataset_characteristics() is responsible to split the datasets of OpenML-CC18 into\n",
    "- Disbalanced binary tasks\n",
    "- Balanced binary tasks\n",
    "- Multiclasstasks\n",
    "  \n",
    "Futhermore, in this function, we can filter datasets by it number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset_characteristics(benchmark: str =\"OpenML-CC18\", disbalance_threshold: float = 0.3) -> dict:\n",
    "    benchmark_suite = openml.study.get_suite(benchmark)\n",
    "    subset_benchmark_suite = benchmark_suite.tasks[0:50]\n",
    "    disbalanced_binary_tasks = []\n",
    "    balanced_binary_tasks = []\n",
    "    multiclass_tasks = []\n",
    "\n",
    "    for task_id in subset_benchmark_suite:\n",
    "        task = openml.tasks.get_task(task_id)\n",
    "        _, targets = task.get_X_and_y()\n",
    "        num_classes = len(np.unique(targets))\n",
    "        \n",
    "        num_instances = len(targets)\n",
    "        if num_instances > 5000:\n",
    "            print(f\"Dataset {task_id} too big. Discarted\")\n",
    "            continue\n",
    "\n",
    "        if num_classes == 2:  # Binary classification task\n",
    "            minority_fraction = pd.Series(targets).value_counts(normalize=True).min()\n",
    "            if minority_fraction < disbalance_threshold:  disbalanced_binary_tasks.append(task_id)\n",
    "            else: balanced_binary_tasks.append(task_id)\n",
    "            continue\n",
    "\n",
    "        multiclass_tasks.append(task_id)\n",
    "\n",
    "    return {\n",
    "        \"disbalanced_binary_tasks\": disbalanced_binary_tasks,\n",
    "        \"balanced_binary_tasks\": balanced_binary_tasks,\n",
    "        \"multiclass_tasks\": multiclass_tasks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = separate_dataset_characteristics()\n",
    "print(\"Disbalanced binary tasks: \", tasks['disbalanced_binary_tasks'])\n",
    "print(\"Balanced binary tasks: \", tasks['balanced_binary_tasks'])\n",
    "print(\"Multiclass tasks: \", tasks['multiclass_tasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The function filter_columns() is responsible for selecting the key metrics essential for generating the CSV. It is optional. built just to facilitate the analysis process by focusing on pertinent information. In practice, it filters the metrics for each fold of cross-validation and retains only the mean metrics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(results: DataFrame) -> DataFrame:\n",
    "    keep_columns = ['Model', 'Dataset', 'mean_test_accuracy', 'mean_test_precision', 'mean_test_recall',\n",
    "                    'mean_test_f1', 'mean_test_roc_auc_ovr', 'Data_type']\n",
    "    keep_columns += [col for col in results.columns if col.startswith('param_')]\n",
    "    results = results[keep_columns]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "The function run_benchmark() trains and evaluates all subsets of models using different hyperparameters. It then incorporates metrics, the number of tasks, dataset type (extracted separately using separate_dataset_characteristics()), and the analyzed model into a dataframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model: any, model_name: str, params: dict = None, metrics: list = None, tasks: list = None, tasks_description: str = None) -> DataFrame:\n",
    "    print(f\"\\nEvaluating metric for {model_name} model\")\n",
    "    results_list = []\n",
    "\n",
    "    if tasks is None or tasks == []: return \n",
    "\n",
    "    for task_id in tasks:\n",
    "        print(f\"Started task {task_id}\")\n",
    "        task = openml.tasks.get_task(task_id)\n",
    "        features, targets = task.get_X_and_y()\n",
    "        \n",
    "        grid_search = GridSearchCV(model, params, cv=10, scoring=metrics, refit=False, n_jobs=-1)\n",
    "        grid_search.fit(features, targets)\n",
    "\n",
    "        results = pd.DataFrame(grid_search.cv_results_)\n",
    "        results['Dataset'] = task_id\n",
    "        results['Data_type'] = tasks_description\n",
    "        results['Model'] = model_name\n",
    "        results_list.append(results)\n",
    "\n",
    "    all_results = pd.concat(results_list, ignore_index=True)\n",
    "    print(\"Finalized evaluation\\n\")\n",
    "    return filter_columns(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_list_of_dataframes(list_of_dataframes: list) -> DataFrame:\n",
    "    if list_of_dataframes: return pd.concat(list_of_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_csv(dataframe, name) -> None:\n",
    "    path = os.path.join(os.getcwd(), '../csv_files/')\n",
    "    if not dataframe.empty:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        full_path = os.path.join(path, name)\n",
    "        dataframe.to_csv(full_path, index=False)\n",
    "        print(f\"CSV file '{full_path}' saved successfully.\")\n",
    "    else:\n",
    "        print(\"No dataframe provided.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Here, we instantiate the models to run the benchmark, then select the metrics for evaluation and the hyperparameters of each model to be combined in gridSearchCV.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = make_pipeline(SimpleImputer(strategy='constant'),DecisionTreeClassifier()) # Decision trees are not sensible to non scaling values\n",
    "knn = make_pipeline(SimpleImputer(strategy='constant'),StandardScaler(),KNeighborsClassifier())\n",
    "svm = make_pipeline(SimpleImputer(strategy='constant'),StandardScaler(),SVC())\n",
    "\n",
    "dt = make_pipeline(SimpleImputer(strategy='constant'),DecisionTreeClassifier()) # Decision trees are not sensible to non scaling values\n",
    "knn = make_pipeline(SimpleImputer(strategy='constant'),StandardScaler(),KNeighborsClassifier())\n",
    "svm = make_pipeline(SimpleImputer(strategy='constant'),StandardScaler(),SVC(probability=True))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc_ovr']\n",
    "models = [dt, knn, svm]\n",
    "model_names = ['dt', 'knn', 'svm']\n",
    "\n",
    "params = {\n",
    "    'dt': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy'],  \n",
    "        'decisiontreeclassifier__max_depth': [5, 7, 9],  \n",
    "        'decisiontreeclassifier__min_samples_split': [3, 4, 5],  \n",
    "        'decisiontreeclassifier__min_samples_leaf': [2, 3, 4]\n",
    "    },\n",
    "    'knn': {\n",
    "        'kneighborsclassifier__n_neighbors': [3, 5, 7, 9, 11],  \n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],  \n",
    "    },\n",
    "    'svm': {\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  \n",
    "        'svc__gamma': ['scale', 'auto']  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Finally, we run the benchmark, generating three CSV files, one for each model. Each CSV contains information on metrics, tasks, and parameters for each subset of datasets that we split at the beginning, such as balanced binary, imbalanced binary, and multiclass. It's important to note that we set gridSearchCV to run with n_jobs=-1, utilizing all available cores to run in parallel. However, even with this setup, calculating all the metrics for SVMs on a large quantity of datasets consumes significant time and resources. Therefore, we analyze only a portion of the datasets to observe the nature of SVMs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        for model, model_name in zip(models, model_names):\n",
    "            metric_results={}\n",
    "            print(f\"\\nModel Name: {model_name}\")\n",
    "            print(f\"Keys in params: {params.keys()}\")\n",
    "            results = []\n",
    "\n",
    "            results.append(run_benchmark(model, model_name, params=params[model_name], metrics=metrics, tasks=tasks['disbalanced_binary_tasks'], tasks_description='disbalanced_binary_tasks'))\n",
    "            results.append(run_benchmark(model, model_name, params=params[model_name], metrics=metrics, tasks=tasks['balanced_binary_tasks'], tasks_description='balanced_binary_tasks'))\n",
    "            results.append(run_benchmark(model, model_name, params=params[model_name], metrics=metrics, tasks=tasks['multiclass_tasks'], tasks_description='multiclass_tasks'))\n",
    "            metric_results[model_name] = results\n",
    "\n",
    "            concatenated_df = concat_list_of_dataframes(metric_results[model_name])\n",
    "            if not concatenated_df.empty:\n",
    "                create_csv(concatenated_df, f\"metrics_{model_name}.csv\")\n",
    "                print(f\"Created csv metrics_{model_name}.csv\")\n",
    "            else:\n",
    "                print(f\"No dataframes to concatenate for {model_name}.\")\n",
    "except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
